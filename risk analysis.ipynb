{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1942970,"sourceType":"datasetVersion","datasetId":1159053},{"sourceId":5306083,"sourceType":"datasetVersion","datasetId":3084682}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport librosa\nimport matplotlib.pyplot as plt\nimport IPython\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation,Reshape,MaxPooling2D, Dropout, Conv2D, MaxPool2D, Flatten\nfrom tensorflow.keras.utils import to_categorical","metadata":{"execution":{"iopub.status.busy":"2024-10-20T20:21:50.160327Z","iopub.execute_input":"2024-10-20T20:21:50.160738Z","iopub.status.idle":"2024-10-20T20:22:03.548377Z","shell.execute_reply.started":"2024-10-20T20:21:50.160693Z","shell.execute_reply":"2024-10-20T20:22:03.547274Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport librosa\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Reshape\n\n# Define the root directories\nreal_root_dir = '/kaggle/input/the-lj-speech-dataset/LJSpeech-1.1/wavs'\nfake_root_dirs = [\n    '/kaggle/input/wavefake-test/generated_audio/ljspeech_melgan',\n    '/kaggle/input/wavefake-test/generated_audio/ljspeech_melgan_large',\n    '/kaggle/input/wavefake-test/generated_audio/ljspeech_waveglow'\n]\n\ndef load_data():\n    paths = []\n    labels = []\n    \n    # Iterate through the real dataset\n    for filename in os.listdir(real_root_dir):\n        file_path = os.path.join(real_root_dir, filename)\n        paths.append(file_path)\n        labels.append('real')\n\n    # Iterate through all fake directories\n    for fake_root_dir in fake_root_dirs:\n        for filename in os.listdir(fake_root_dir):\n            file_path = os.path.join(fake_root_dir, filename)\n            paths.append(file_path)\n            labels.append('fake')\n\n    print('Dataset is loaded')\n    return paths, labels\n\npaths, labels = load_data()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T20:22:03.550078Z","iopub.execute_input":"2024-10-20T20:22:03.550681Z","iopub.status.idle":"2024-10-20T20:22:05.982942Z","shell.execute_reply.started":"2024-10-20T20:22:03.550645Z","shell.execute_reply":"2024-10-20T20:22:05.981672Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Dataset is loaded\n","output_type":"stream"}]},{"cell_type":"code","source":"def extract_features(fake_root_dir, real_root_dir, max_length=500):\n    features = []\n    labels = []\n    \n    for file in os.listdir(fake_root_dir):\n        file_path = os.path.join(fake_root_dir, file)\n        try:\n            audio, _ = librosa.load(file_path, sr=16000)\n            mfccs = librosa.feature.mfcc(y=audio, sr=16000, n_mfcc=40)\n            if mfccs.shape[1] < max_length:\n                mfccs = np.pad(mfccs, ((0, 0), (0, max_length - mfccs.shape[1])), mode='constant')\n            else:\n                mfccs = mfccs[:, :max_length]\n            features.append(mfccs)\n            labels.append(1)  # 1 for fake\n        except Exception as e:\n            print(f\"Error encountered while parsing file: {file_path}\")\n            continue\n            \n    for file in os.listdir(real_root_dir):\n        file_path = os.path.join(real_root_dir, file)\n        try:\n            audio, _ = librosa.load(file_path, sr=16000)\n            mfccs = librosa.feature.mfcc(y=audio, sr=16000, n_mfcc=40)\n            if mfccs.shape[1] < max_length:\n                mfccs = np.pad(mfccs, ((0, 0), (0, max_length - mfccs.shape[1])), mode='constant')\n            else:\n                mfccs = mfccs[:, :max_length]\n            features.append(mfccs)\n            labels.append(0)  # 0 for real\n        except Exception as e:\n            print(f\"Error encountered while parsing file: {file_path}\")\n            continue\n    return np.array(features), np.array(labels)\n\n# Example usage\nx, y = extract_features(fake_root_dirs[0], real_root_dir)  # Using one fake dir for demo\nprint(\"Features shape:\", x.shape)\nprint(\"Labels shape:\", y.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T20:22:05.984345Z","iopub.execute_input":"2024-10-20T20:22:05.984768Z","iopub.status.idle":"2024-10-20T20:36:38.966709Z","shell.execute_reply.started":"2024-10-20T20:22:05.984720Z","shell.execute_reply":"2024-10-20T20:36:38.964625Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Features shape: (26200, 40, 500)\nLabels shape: (26200,)\n","output_type":"stream"}]},{"cell_type":"code","source":"def generate_counterfactuals(features):\n    counterfactuals = []\n    for mfcc in features:\n        noise = np.random.normal(0, 0.01, mfcc.shape)  # Add Gaussian noise\n        counterfactuals.append(mfcc + noise)\n        # You can add more transformations if desired\n    return np.array(counterfactuals)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T20:36:38.971870Z","iopub.execute_input":"2024-10-20T20:36:38.973605Z","iopub.status.idle":"2024-10-20T20:36:38.981685Z","shell.execute_reply.started":"2024-10-20T20:36:38.973556Z","shell.execute_reply":"2024-10-20T20:36:38.980491Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Generate original training features and labels\noriginal_features, original_labels = extract_features(fake_root_dirs[0], real_root_dir)\n\n# Split original features into training and test sets\nxtrain, xtest, ytrain, ytest = train_test_split(original_features, original_labels, test_size=0.3, random_state=42)\n\n# Generate counterfactual samples\ncounterfactuals = generate_counterfactuals(xtrain)\n\n# Combine original and counterfactual features and labels\nx_combined = np.concatenate((xtrain, counterfactuals))\ny_combined = np.concatenate((ytrain, ytrain))  # Same labels for counterfactuals\n\n# Reshape the combined input to add channel dimension for the model\nx_combined = x_combined.reshape((-1, 40, 500, 1))  # Reshape to match the input shape of the model\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T20:36:38.983007Z","iopub.execute_input":"2024-10-20T20:36:38.983512Z","iopub.status.idle":"2024-10-20T20:48:22.923528Z","shell.execute_reply.started":"2024-10-20T20:36:38.983464Z","shell.execute_reply":"2024-10-20T20:48:22.921967Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Build the model\nmodel = Sequential([\n    Reshape((40, 500, 1), input_shape=x_combined.shape[1:]),  # Adjust input shape\n    Conv2D(32, kernel_size=(3, 3), activation='relu'),\n    MaxPooling2D(pool_size=(2, 2)),\n    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n    MaxPooling2D(pool_size=(2, 2)),\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nhistory = model.fit(x_combined, y_combined, epochs=20, batch_size=32, validation_data=(xtest.reshape((-1, 40, 500, 1)), ytest))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T20:48:22.925713Z","iopub.execute_input":"2024-10-20T20:48:22.926191Z","iopub.status.idle":"2024-10-20T23:33:02.552603Z","shell.execute_reply.started":"2024-10-20T20:48:22.926143Z","shell.execute_reply":"2024-10-20T23:33:02.549016Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/reshaping/reshape.py:39: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m504s\u001b[0m 438ms/step - accuracy: 0.5519 - loss: 2.7011 - val_accuracy: 0.7716 - val_loss: 0.4898\nEpoch 2/20\n\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m561s\u001b[0m 437ms/step - accuracy: 0.8094 - loss: 0.4142 - val_accuracy: 0.8813 - val_loss: 0.2784\nEpoch 3/20\n\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 433ms/step - accuracy: 0.9019 - loss: 0.2332 - val_accuracy: 0.9060 - val_loss: 0.2256\nEpoch 4/20\n\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m493s\u001b[0m 430ms/step - accuracy: 0.9294 - loss: 0.1673 - val_accuracy: 0.9139 - val_loss: 0.2079\nEpoch 5/20\n\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m489s\u001b[0m 427ms/step - accuracy: 0.9498 - loss: 0.1241 - val_accuracy: 0.9097 - val_loss: 0.2475\nEpoch 6/20\n\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m501s\u001b[0m 425ms/step - accuracy: 0.9621 - loss: 0.0960 - val_accuracy: 0.9182 - val_loss: 0.2131\nEpoch 7/20\n\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 434ms/step - accuracy: 0.9731 - loss: 0.0712 - val_accuracy: 0.9089 - val_loss: 0.2577\nEpoch 8/20\n\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m488s\u001b[0m 425ms/step - accuracy: 0.9690 - loss: 0.0792 - val_accuracy: 0.9206 - val_loss: 0.2522\nEpoch 9/20\n\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m503s\u001b[0m 426ms/step - accuracy: 0.9798 - loss: 0.0572 - val_accuracy: 0.9197 - val_loss: 0.2505\nEpoch 10/20\n\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m486s\u001b[0m 423ms/step - accuracy: 0.9800 - loss: 0.0524 - val_accuracy: 0.9221 - val_loss: 0.2534\nEpoch 11/20\n\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m482s\u001b[0m 420ms/step - accuracy: 0.9828 - loss: 0.0465 - val_accuracy: 0.9126 - val_loss: 0.3325\nEpoch 12/20\n\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m483s\u001b[0m 421ms/step - accuracy: 0.9821 - loss: 0.0495 - val_accuracy: 0.8976 - val_loss: 0.5017\nEpoch 13/20\n\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m479s\u001b[0m 418ms/step - accuracy: 0.9648 - loss: 0.0966 - val_accuracy: 0.9094 - val_loss: 0.3502\nEpoch 14/20\n\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m478s\u001b[0m 416ms/step - accuracy: 0.9867 - loss: 0.0386 - val_accuracy: 0.9130 - val_loss: 0.4685\nEpoch 15/20\n\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m479s\u001b[0m 418ms/step - accuracy: 0.9841 - loss: 0.0471 - val_accuracy: 0.8948 - val_loss: 0.4492\nEpoch 16/20\n\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m482s\u001b[0m 420ms/step - accuracy: 0.9850 - loss: 0.0420 - val_accuracy: 0.9046 - val_loss: 0.5127\nEpoch 17/20\n\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m481s\u001b[0m 419ms/step - accuracy: 0.9875 - loss: 0.0369 - val_accuracy: 0.9144 - val_loss: 0.3097\nEpoch 18/20\n\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m479s\u001b[0m 418ms/step - accuracy: 0.9890 - loss: 0.0297 - val_accuracy: 0.9204 - val_loss: 0.3070\nEpoch 19/20\n\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m504s\u001b[0m 419ms/step - accuracy: 0.9878 - loss: 0.0351 - val_accuracy: 0.9065 - val_loss: 0.4290\nEpoch 20/20\n\u001b[1m1147/1147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m500s\u001b[0m 418ms/step - accuracy: 0.9882 - loss: 0.0339 - val_accuracy: 0.8948 - val_loss: 0.4298\n","output_type":"stream"}]},{"cell_type":"code","source":"# Generate counterfactuals for test set\ncounterfactual_test = generate_counterfactuals(xtest)\n\n# Reshape the counterfactual test data to match the input shape expected by the model\ncounterfactual_test = counterfactual_test.reshape((-1, 40, 500, 1))\n\n# Get predictions for original test data\noriginal_predictions = model.predict(xtest.reshape((-1, 40, 500, 1)))\n\n# Get predictions for counterfactual test data\ncounterfactual_predictions = model.predict(counterfactual_test)\n\n# Analyze changes in predictions\nsensitivity_analysis = np.abs(original_predictions - counterfactual_predictions)\n\n# Risk estimation can be done based on how much the predictions change\nrisk_threshold = 0.5  # Example threshold\nrisk_estimate = sensitivity_analysis > risk_threshold\nprint(\"Risk estimation based on sensitivity analysis:\", risk_estimate)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T23:33:02.559290Z","iopub.execute_input":"2024-10-20T23:33:02.560148Z","iopub.status.idle":"2024-10-20T23:34:17.376777Z","shell.execute_reply.started":"2024-10-20T23:33:02.560076Z","shell.execute_reply":"2024-10-20T23:34:17.375560Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 126ms/step\n\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 110ms/step\nRisk estimation based on sensitivity analysis: [[False]\n [False]\n [False]\n ...\n [False]\n [False]\n [False]]\n","output_type":"stream"}]}]}